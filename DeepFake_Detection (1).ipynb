{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs_QHeP1VbfC"
      },
      "outputs": [],
      "source": [
        "# Importing basic libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Deep learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, TimeDistributed\n",
        "from tensorflow.keras.layers import Bidirectional, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Additional libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XuQMXvRH5yj",
        "outputId": "c5925581-a3da-4e2b-bee9-e6f5ba039a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucoFywnGXATA"
      },
      "outputs": [],
      "source": [
        "real_videos_path = '/content/drive/MyDrive/Celeb-DF/Celeb-real'\n",
        "fake_videos_path = '/content/drive/MyDrive/Celeb-DF/Celeb-synthesis'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvtiGvP8WGvu"
      },
      "outputs": [],
      "source": [
        "def load_data(data_dir, label, max_videos=100):\n",
        "    \"\"\"Load video data and return frames with labels.\"\"\"\n",
        "    videos, labels = [], []\n",
        "    for i, filename in enumerate(os.listdir(data_dir)):\n",
        "        if i == max_videos:\n",
        "            break\n",
        "        filepath = os.path.join(data_dir, filename)\n",
        "        frames = extract_frames(filepath)\n",
        "        if len(frames) == 30:  # Ensuring all videos have the same number of frames\n",
        "            videos.append(frames)\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipped {filename}: Not enough frames or corrupted video.\")\n",
        "    return np.array(videos), np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wai5E4WWQBR"
      },
      "outputs": [],
      "source": [
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    \"\"\"Extract frames from a video file.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    if len(frames) < max_frames:\n",
        "        print(f\"Warning: {video_path} has only {len(frames)} frames.\")\n",
        "    return np.array(frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DojatALWZL6",
        "outputId": "b221f2ec-e9b5-4121-91b4-570d96612727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real videos loaded: 100\n",
            "Fake videos loaded: 100\n"
          ]
        }
      ],
      "source": [
        "real_videos, real_labels = load_data(real_videos_path, 0)\n",
        "fake_videos, fake_labels = load_data(fake_videos_path, 1)\n",
        "\n",
        "# Check data sizes\n",
        "print(f\"Real videos loaded: {real_videos.shape[0]}\")\n",
        "print(f\"Fake videos loaded: {fake_videos.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th87RKU8WbvY"
      },
      "outputs": [],
      "source": [
        "# Check if any videos are loaded\n",
        "if len(real_videos) == 0 or len(fake_videos) == 0:\n",
        "    raise ValueError(\"No video data loaded. Check your file paths and data loading functions.\")\n",
        "\n",
        "# Combine real and fake videos\n",
        "X = np.concatenate((real_videos, fake_videos))\n",
        "y = np.concatenate((real_labels, fake_labels))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "# Adjust test_size or max_videos if there are not enough samples\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF_RAqdhXadv"
      },
      "outputs": [],
      "source": [
        "def build_cnn_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(30, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Correct output for binary classification\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TErZ2nkePQC"
      },
      "outputs": [],
      "source": [
        "m=build_cnn_lstm_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib_BcogYXgGY"
      },
      "outputs": [],
      "source": [
        "m.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClFcoz8oXmb0",
        "outputId": "b30fadec-9bc0-49b5-97b9-9fdb653192fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real videos loaded: 100\n",
            "Fake videos loaded: 100\n",
            "y shape before split: (200,)\n",
            "X_train shape: (160, 30, 224, 224, 3)\n",
            "y_train shape: (160,)\n",
            "X_test shape: (40, 30, 224, 224, 3)\n",
            "y_test shape: (40,)\n",
            "Epoch 1/50\n",
            "20/20 [==============================] - 579s 29s/step - loss: 0.7893 - accuracy: 0.4375 - val_loss: 0.6958 - val_accuracy: 0.4750\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 557s 28s/step - loss: 0.6988 - accuracy: 0.5250 - val_loss: 0.6938 - val_accuracy: 0.4750\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 556s 28s/step - loss: 0.7040 - accuracy: 0.5750 - val_loss: 0.6920 - val_accuracy: 0.5250\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 554s 28s/step - loss: 0.7424 - accuracy: 0.4688 - val_loss: 0.6967 - val_accuracy: 0.4750\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 557s 28s/step - loss: 0.7153 - accuracy: 0.5063 - val_loss: 0.6961 - val_accuracy: 0.4750\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 555s 28s/step - loss: 0.7404 - accuracy: 0.4688 - val_loss: 0.6942 - val_accuracy: 0.4750\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 556s 28s/step - loss: 0.7118 - accuracy: 0.4688 - val_loss: 0.6943 - val_accuracy: 0.4750\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 557s 28s/step - loss: 0.7010 - accuracy: 0.4563 - val_loss: 0.6927 - val_accuracy: 0.5000\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 553s 28s/step - loss: 0.7368 - accuracy: 0.4500 - val_loss: 0.6948 - val_accuracy: 0.4750\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 555s 28s/step - loss: 0.7331 - accuracy: 0.4250 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 554s 28s/step - loss: 0.7144 - accuracy: 0.4875 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 555s 28s/step - loss: 0.7026 - accuracy: 0.4875 - val_loss: 0.6916 - val_accuracy: 0.5250\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 554s 28s/step - loss: 0.7104 - accuracy: 0.4187 - val_loss: 0.6926 - val_accuracy: 0.5000\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 553s 28s/step - loss: 0.7068 - accuracy: 0.5063 - val_loss: 0.6968 - val_accuracy: 0.4750\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 554s 28s/step - loss: 0.7219 - accuracy: 0.4688 - val_loss: 0.6917 - val_accuracy: 0.5250\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 553s 28s/step - loss: 0.7110 - accuracy: 0.4375 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 552s 28s/step - loss: 0.7005 - accuracy: 0.5125 - val_loss: 0.6963 - val_accuracy: 0.4750\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 552s 28s/step - loss: 0.6955 - accuracy: 0.5125 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 553s 28s/step - loss: 0.7001 - accuracy: 0.4875 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 552s 28s/step - loss: 0.7030 - accuracy: 0.4750 - val_loss: 0.6962 - val_accuracy: 0.4750\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 552s 28s/step - loss: 0.7047 - accuracy: 0.4437 - val_loss: 0.6923 - val_accuracy: 0.5250\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 551s 28s/step - loss: 0.6969 - accuracy: 0.4625 - val_loss: 0.6924 - val_accuracy: 0.5250\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 553s 28s/step - loss: 0.7069 - accuracy: 0.4500 - val_loss: 0.6951 - val_accuracy: 0.5000\n",
            "Epoch 24/50\n",
            "12/20 [=================>............] - ETA: 3:30 - loss: 0.7019 - accuracy: 0.4792"
          ]
        }
      ],
      "source": [
        "\n",
        "real_videos, real_labels = load_data(real_videos_path, 0)\n",
        "fake_videos, fake_labels = load_data(fake_videos_path, 1)\n",
        "\n",
        "\n",
        "print(f\"Real videos loaded: {real_videos.shape[0]}\")\n",
        "print(f\"Fake videos loaded: {fake_videos.shape[0]}\")\n",
        "\n",
        "\n",
        "if len(real_videos) == 0 or len(fake_videos) == 0:\n",
        "    raise ValueError(\"No video data loaded. Check your file paths and data loading functions.\")\n",
        "\n",
        "X = np.concatenate((real_videos, fake_videos))\n",
        "y = np.concatenate((real_labels, fake_labels))\n",
        "\n",
        "\n",
        "print(f\"y shape before split: {y.shape}\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "\n",
        "def build_cnn_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(30, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "model = build_cnn_lstm_model()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5tNL1trisT7"
      },
      "outputs": [],
      "source": [
        "# Print the final training accuracy\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "print(f\"Final Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the final validation (testing) accuracy\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Validation (Test) Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history('accuracy'))\n",
        "plt.plot(history.history('val_accuracy'))\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend([\"Train\",\"Test\"],loc=\"upper\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history('loss'))\n",
        "plt.plot(history.history('val_loss'))\n",
        "plt.title(\"Model Loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend([\"Train\",\"Test\"],loc=\"upper\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EdZas2fL16RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SC95ObbaDte"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ytuH2X3alv2",
        "outputId": "d74772e3-05f3-46cf-a739-66630fcf65d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "20/20 [==============================] - 6s 319ms/step - loss: 0.6899 - accuracy: 0.4563 - val_loss: 0.6941 - val_accuracy: 0.4750\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 6s 286ms/step - loss: 0.6805 - accuracy: 0.5625 - val_loss: 0.6937 - val_accuracy: 0.4750\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 6s 285ms/step - loss: 0.6815 - accuracy: 0.4938 - val_loss: 0.6925 - val_accuracy: 0.5250\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 6s 305ms/step - loss: 0.6827 - accuracy: 0.5437 - val_loss: 0.6935 - val_accuracy: 0.4750\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 6s 300ms/step - loss: 0.6843 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5250\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 6s 307ms/step - loss: 0.6901 - accuracy: 0.4688 - val_loss: 0.6944 - val_accuracy: 0.4750\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 6s 291ms/step - loss: 0.6788 - accuracy: 0.5312 - val_loss: 0.6931 - val_accuracy: 0.5250\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 6s 295ms/step - loss: 0.6875 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4750\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 6s 301ms/step - loss: 0.6869 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.5250\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 6s 287ms/step - loss: 0.6803 - accuracy: 0.5312 - val_loss: 0.6926 - val_accuracy: 0.5250\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 6s 291ms/step - loss: 0.6823 - accuracy: 0.5250 - val_loss: 0.6930 - val_accuracy: 0.5250\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 6s 295ms/step - loss: 0.6893 - accuracy: 0.4938 - val_loss: 0.6937 - val_accuracy: 0.4750\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 6s 299ms/step - loss: 0.6814 - accuracy: 0.5312 - val_loss: 0.6919 - val_accuracy: 0.5250\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 6s 294ms/step - loss: 0.6839 - accuracy: 0.4750 - val_loss: 0.6921 - val_accuracy: 0.5250\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 6s 287ms/step - loss: 0.6733 - accuracy: 0.5312 - val_loss: 0.6928 - val_accuracy: 0.5250\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 6s 289ms/step - loss: 0.6802 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.5250\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 6s 297ms/step - loss: 0.6843 - accuracy: 0.5125 - val_loss: 0.6920 - val_accuracy: 0.5250\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 6s 292ms/step - loss: 0.6846 - accuracy: 0.5375 - val_loss: 0.6923 - val_accuracy: 0.5250\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 6s 298ms/step - loss: 0.6845 - accuracy: 0.5312 - val_loss: 0.6929 - val_accuracy: 0.5250\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 6s 300ms/step - loss: 0.6865 - accuracy: 0.4187 - val_loss: 0.6929 - val_accuracy: 0.5250\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 6s 291ms/step - loss: 0.6744 - accuracy: 0.6000 - val_loss: 0.6923 - val_accuracy: 0.5250\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 6s 297ms/step - loss: 0.6824 - accuracy: 0.5375 - val_loss: 0.6926 - val_accuracy: 0.5250\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 6s 293ms/step - loss: 0.6734 - accuracy: 0.5250 - val_loss: 0.6929 - val_accuracy: 0.5250\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 6s 299ms/step - loss: 0.6704 - accuracy: 0.5750 - val_loss: 0.6926 - val_accuracy: 0.5250\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 6s 303ms/step - loss: 0.6806 - accuracy: 0.5437 - val_loss: 0.6950 - val_accuracy: 0.4750\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 6s 285ms/step - loss: 0.6872 - accuracy: 0.5188 - val_loss: 0.6930 - val_accuracy: 0.5250\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 6s 290ms/step - loss: 0.6821 - accuracy: 0.5125 - val_loss: 0.6919 - val_accuracy: 0.5250\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 6s 302ms/step - loss: 0.6768 - accuracy: 0.5688 - val_loss: 0.6924 - val_accuracy: 0.5250\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 6s 285ms/step - loss: 0.6858 - accuracy: 0.4437 - val_loss: 0.6927 - val_accuracy: 0.5250\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 6s 292ms/step - loss: 0.6837 - accuracy: 0.5250 - val_loss: 0.6922 - val_accuracy: 0.5250\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 6s 284ms/step - loss: 0.6850 - accuracy: 0.4938 - val_loss: 0.6928 - val_accuracy: 0.5250\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 6s 291ms/step - loss: 0.6788 - accuracy: 0.4875 - val_loss: 0.6922 - val_accuracy: 0.5250\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 6s 300ms/step - loss: 0.6786 - accuracy: 0.5312 - val_loss: 0.6924 - val_accuracy: 0.5250\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 6s 285ms/step - loss: 0.6775 - accuracy: 0.5125 - val_loss: 0.6925 - val_accuracy: 0.5250\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 6s 310ms/step - loss: 0.6867 - accuracy: 0.4563 - val_loss: 0.6928 - val_accuracy: 0.5250\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 6s 285ms/step - loss: 0.6736 - accuracy: 0.5688 - val_loss: 0.6920 - val_accuracy: 0.5250\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 6s 298ms/step - loss: 0.6767 - accuracy: 0.5500 - val_loss: 0.6921 - val_accuracy: 0.5250\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 6s 299ms/step - loss: 0.6832 - accuracy: 0.4750 - val_loss: 0.6928 - val_accuracy: 0.5250\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 6s 286ms/step - loss: 0.6745 - accuracy: 0.4875 - val_loss: 0.6930 - val_accuracy: 0.5250\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 6s 304ms/step - loss: 0.6852 - accuracy: 0.5188 - val_loss: 0.6939 - val_accuracy: 0.4750\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 6s 286ms/step - loss: 0.6794 - accuracy: 0.4812 - val_loss: 0.6941 - val_accuracy: 0.4750\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 6s 291ms/step - loss: 0.6774 - accuracy: 0.5188 - val_loss: 0.6931 - val_accuracy: 0.5250\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 6s 287ms/step - loss: 0.6812 - accuracy: 0.4875 - val_loss: 0.6926 - val_accuracy: 0.5250\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 6s 302ms/step - loss: 0.6725 - accuracy: 0.5562 - val_loss: 0.6926 - val_accuracy: 0.5250\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 6s 288ms/step - loss: 0.6806 - accuracy: 0.5375 - val_loss: 0.6922 - val_accuracy: 0.5250\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 6s 286ms/step - loss: 0.6715 - accuracy: 0.5625 - val_loss: 0.6924 - val_accuracy: 0.5250\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 6s 296ms/step - loss: 0.6781 - accuracy: 0.5063 - val_loss: 0.6923 - val_accuracy: 0.5250\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 6s 297ms/step - loss: 0.6832 - accuracy: 0.4437 - val_loss: 0.6922 - val_accuracy: 0.5250\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 6s 288ms/step - loss: 0.6817 - accuracy: 0.5500 - val_loss: 0.6922 - val_accuracy: 0.5250\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 6s 294ms/step - loss: 0.6793 - accuracy: 0.5437 - val_loss: 0.6921 - val_accuracy: 0.5250\n",
            "Final Training Accuracy: 54.37%\n",
            "Final Validation (Test) Accuracy: 52.50%\n"
          ]
        }
      ],
      "source": [
        "# Train the model and capture the history\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test))\n",
        "\n",
        "# Print the final training accuracy\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "print(f\"Final Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the final validation (testing) accuracy\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Validation (Test) Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-A5qc-4a7ai"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsnUStmbSDFO",
        "outputId": "2936bf3b-cab6-4c59-d12c-c7628738d76f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 140ms/step\n",
            "The uploaded video is predicted to be: Real\n"
          ]
        }
      ],
      "source": [
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    \"\"\"Extract frames from the uploaded video and preprocess for model prediction.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "\n",
        "    # Ensure we have the required number of frames by padding if necessary\n",
        "    if len(frames) < max_frames:\n",
        "        print(f\"Warning: {video_path} has only {len(frames)} frames. Padding with black frames.\")\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Add blank frame\n",
        "\n",
        "    return np.array(frames)\n",
        "\n",
        "def predict_video(video_path):\n",
        "    frames = extract_frames(video_path)\n",
        "    frames = frames[np.newaxis, ...]  # Add batch dimension\n",
        "    prediction = model.predict(frames)\n",
        "    return \"Fake\" if prediction[0][0] > 0.5 else \"Real\"\n",
        "\n",
        "# Testing with a video\n",
        "uploaded_video_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-synthesis/id0_id16_0000.mp4'  # Change to your uploaded video's path\n",
        "result = predict_video(uploaded_video_path)\n",
        "print(f\"The uploaded video is predicted to be: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SszzlwxGSyaA",
        "outputId": "a8edab71-efe5-45ca-8043-a12f3a80554c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 140ms/step\n",
            "The uploaded video is predicted to be: Fake\n"
          ]
        }
      ],
      "source": [
        "# Testing with a video\n",
        "uploaded_video_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id0_0003.mp4'  # Change to your uploaded video's path\n",
        "result = predict_video(uploaded_video_path)\n",
        "print(f\"The uploaded video is predicted to be: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcfD9GEYVSpE",
        "outputId": "2faeba99-d6f3-4699-d71c-cc2da4cafe1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model 1\n",
            "Epoch 1/10\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.7635 - accuracy: 0.4625 - val_loss: 0.6971 - val_accuracy: 0.4750\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 76s 4s/step - loss: 0.7363 - accuracy: 0.4812 - val_loss: 0.6969 - val_accuracy: 0.4750\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 75s 4s/step - loss: 0.7089 - accuracy: 0.4812 - val_loss: 0.7035 - val_accuracy: 0.4750\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 75s 4s/step - loss: 0.6882 - accuracy: 0.5562 - val_loss: 0.6982 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 75s 4s/step - loss: 0.7117 - accuracy: 0.4625 - val_loss: 0.7037 - val_accuracy: 0.4750\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 76s 4s/step - loss: 0.7055 - accuracy: 0.4688 - val_loss: 0.6975 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 76s 4s/step - loss: 0.6966 - accuracy: 0.5250 - val_loss: 0.6969 - val_accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.7045 - accuracy: 0.5125 - val_loss: 0.6991 - val_accuracy: 0.4750\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.7004 - accuracy: 0.4625 - val_loss: 0.6984 - val_accuracy: 0.4750\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.7114 - accuracy: 0.4375 - val_loss: 0.6984 - val_accuracy: 0.4750\n",
            "Final Validation Accuracy for Model 1: 47.50%\n",
            "\n",
            "Training Model 2\n",
            "Epoch 1/10\n",
            "20/20 [==============================] - 123s 6s/step - loss: 0.7847 - accuracy: 0.4688 - val_loss: 0.7010 - val_accuracy: 0.4500\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.7354 - accuracy: 0.4750 - val_loss: 0.7038 - val_accuracy: 0.5250\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.7779 - accuracy: 0.4812 - val_loss: 0.6968 - val_accuracy: 0.4750\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.7272 - accuracy: 0.4812 - val_loss: 0.6976 - val_accuracy: 0.4250\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.7044 - accuracy: 0.5312 - val_loss: 0.6933 - val_accuracy: 0.5250\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 115s 6s/step - loss: 0.7156 - accuracy: 0.4875 - val_loss: 0.6981 - val_accuracy: 0.4750\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 120s 6s/step - loss: 0.7293 - accuracy: 0.4750 - val_loss: 0.6909 - val_accuracy: 0.5500\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 122s 6s/step - loss: 0.7213 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.4750\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 123s 6s/step - loss: 0.6893 - accuracy: 0.5500 - val_loss: 0.6924 - val_accuracy: 0.6000\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 123s 6s/step - loss: 0.7051 - accuracy: 0.5312 - val_loss: 0.6927 - val_accuracy: 0.5500\n",
            "Final Validation Accuracy for Model 2: 55.00%\n",
            "\n",
            "Training Model 3\n",
            "Epoch 1/10\n",
            "20/20 [==============================] - 113s 5s/step - loss: 0.7434 - accuracy: 0.4938 - val_loss: 0.6944 - val_accuracy: 0.5250\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.6970 - accuracy: 0.5375 - val_loss: 0.6910 - val_accuracy: 0.5250\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.7257 - accuracy: 0.4812 - val_loss: 0.6936 - val_accuracy: 0.5250\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.7077 - accuracy: 0.4812 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.7303 - accuracy: 0.4625 - val_loss: 0.6962 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 109s 5s/step - loss: 0.6872 - accuracy: 0.5688 - val_loss: 0.6907 - val_accuracy: 0.5250\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 110s 6s/step - loss: 0.7029 - accuracy: 0.5250 - val_loss: 0.7005 - val_accuracy: 0.4750\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 110s 6s/step - loss: 0.7177 - accuracy: 0.4625 - val_loss: 0.6916 - val_accuracy: 0.5250\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 111s 6s/step - loss: 0.7603 - accuracy: 0.4250 - val_loss: 0.6982 - val_accuracy: 0.4750\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 111s 6s/step - loss: 0.7296 - accuracy: 0.3812 - val_loss: 0.6910 - val_accuracy: 0.5250\n",
            "Final Validation Accuracy for Model 3: 52.50%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5b5ab03250> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 481ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5b5b38df30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 1s 903ms/step\n",
            "The uploaded video is predicted to be: Real\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import Xception\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Model 1: CNN-LSTM\n",
        "def build_cnn_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(30, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Model 2: Xception-based CNN-LSTM\n",
        "def build_xception_lstm_model():\n",
        "    base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(base_model, input_shape=(30, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Model 3: Bidirectional-LSTM\n",
        "def build_bidirectional_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(30, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Build and compile models\n",
        "model1 = build_cnn_lstm_model()\n",
        "model2 = build_xception_lstm_model()\n",
        "model3 = build_bidirectional_lstm_model()\n",
        "\n",
        "models = [model1, model2, model3]\n",
        "\n",
        "for model in models:\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load pre-trained weights if available\n",
        "# for i, model in enumerate(models):\n",
        "#     model.load_weights(f'model_{i}_weights.h5')\n",
        "\n",
        "# Frame extraction function\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    \"\"\"Extract frames from the uploaded video and preprocess for model prediction.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "\n",
        "    # Ensure we have the required number of frames by padding if necessary\n",
        "    if len(frames) < max_frames:\n",
        "        print(f\"Warning: {video_path} has only {len(frames)} frames. Padding with black frames.\")\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Add blank frame\n",
        "\n",
        "    return np.array(frames)\n",
        "\n",
        "# Placeholder for training dataset\n",
        "# Replace X_train, X_val, y_train, y_val with actual data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train each model for 10 epochs\n",
        "epochs = 10\n",
        "for i, model in enumerate(models):\n",
        "    print(f\"Training Model {i+1}\")\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=8, validation_data=(X_val, y_val))\n",
        "\n",
        "    # Optionally, save weights after training\n",
        "    model.save_weights(f'model_{i}_weights.h5')\n",
        "\n",
        "    # Print final validation accuracy for each model\n",
        "    val_accuracy = history.history['val_accuracy'][-1]\n",
        "    print(f\"Final Validation Accuracy for Model {i+1}: {val_accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "# Ensemble prediction function\n",
        "def predict_video_ensemble(video_path):\n",
        "    frames = extract_frames(video_path)\n",
        "    frames = frames[np.newaxis, ...]  # Add batch dimension\n",
        "\n",
        "    # Get predictions from each model\n",
        "    predictions = [model.predict(frames)[0][0] for model in models]\n",
        "\n",
        "    # Majority vote or average prediction\n",
        "    avg_prediction = np.mean(predictions)\n",
        "    return \"Fake\" if avg_prediction > 0.5 else \"Real\"\n",
        "\n",
        "# Testing with a video\n",
        "uploaded_video_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id0_0003.mp4'  # Change to your uploaded video's path\n",
        "result = predict_video_ensemble(uploaded_video_path)\n",
        "print(f\"The uploaded video is predicted to be: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNC3QUgkilG0",
        "outputId": "b94df457-01d7-4cbb-b5f4-fcca3503e4b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 456ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "The uploaded video is predicted to be: Real\n"
          ]
        }
      ],
      "source": [
        "# Testing with a video\n",
        "uploaded_video_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id0_0003.mp4'  # Change to your uploaded video's path\n",
        "result = predict_video_ensemble(uploaded_video_path)\n",
        "print(f\"The uploaded video is predicted to be: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB9Olrkaj62E",
        "outputId": "d0f02321-fe5c-45a0-d508-d254029f07d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: /content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id10_0004.mp4  has only 0 frames. Padding with black frames.\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 1s 534ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "The uploaded video is predicted to be: Fake\n"
          ]
        }
      ],
      "source": [
        "uploaded_video_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id10_0004.mp4 ' # Change to your uploaded video's path\n",
        "result = predict_video_ensemble(uploaded_video_path)\n",
        "print(f\"The uploaded video is predicted to be: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzjL8Ar4iLL5",
        "outputId": "fe8e0cba-102c-44cb-8477-192b2db5c999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model 1\n",
            "Epoch 1/50\n",
            "20/20 [==============================] - 79s 4s/step - loss: 0.7208 - accuracy: 0.4938 - val_loss: 0.6916 - val_accuracy: 0.5250\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.7436 - accuracy: 0.4250 - val_loss: 0.6908 - val_accuracy: 0.5250\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.7099 - accuracy: 0.5000 - val_loss: 0.6915 - val_accuracy: 0.5750\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6816 - accuracy: 0.5437 - val_loss: 0.6936 - val_accuracy: 0.4750\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.7271 - accuracy: 0.4563 - val_loss: 0.6911 - val_accuracy: 0.5250\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6802 - accuracy: 0.5625 - val_loss: 0.6937 - val_accuracy: 0.4750\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.7009 - accuracy: 0.4875 - val_loss: 0.6935 - val_accuracy: 0.4750\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.7076 - accuracy: 0.4563 - val_loss: 0.6927 - val_accuracy: 0.5750\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.7083 - accuracy: 0.4500 - val_loss: 0.6921 - val_accuracy: 0.5750\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.6892 - accuracy: 0.5500 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.6934 - accuracy: 0.4875 - val_loss: 0.6952 - val_accuracy: 0.4750\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.6922 - accuracy: 0.5063 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.6910 - accuracy: 0.5500 - val_loss: 0.6970 - val_accuracy: 0.4750\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.7014 - accuracy: 0.4625 - val_loss: 0.6921 - val_accuracy: 0.5250\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.6928 - accuracy: 0.5000 - val_loss: 0.6918 - val_accuracy: 0.5250\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6915 - accuracy: 0.5250 - val_loss: 0.6927 - val_accuracy: 0.5250\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6922 - accuracy: 0.5312 - val_loss: 0.6950 - val_accuracy: 0.4750\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6877 - accuracy: 0.5375 - val_loss: 0.6944 - val_accuracy: 0.4750\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6911 - accuracy: 0.5188 - val_loss: 0.6944 - val_accuracy: 0.4750\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 77s 4s/step - loss: 0.6860 - accuracy: 0.5562 - val_loss: 0.6972 - val_accuracy: 0.4750\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6949 - accuracy: 0.4812 - val_loss: 0.6934 - val_accuracy: 0.5250\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6900 - accuracy: 0.5063 - val_loss: 0.6928 - val_accuracy: 0.5250\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6881 - accuracy: 0.5312 - val_loss: 0.6934 - val_accuracy: 0.4750\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6922 - accuracy: 0.5125 - val_loss: 0.6964 - val_accuracy: 0.4750\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6869 - accuracy: 0.5125 - val_loss: 0.6948 - val_accuracy: 0.4750\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6853 - accuracy: 0.5250 - val_loss: 0.6939 - val_accuracy: 0.4500\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6868 - accuracy: 0.5562 - val_loss: 0.6940 - val_accuracy: 0.4250\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6847 - accuracy: 0.5250 - val_loss: 0.6936 - val_accuracy: 0.4500\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6834 - accuracy: 0.5250 - val_loss: 0.6926 - val_accuracy: 0.5250\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6961 - accuracy: 0.4812 - val_loss: 0.6930 - val_accuracy: 0.5250\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6977 - accuracy: 0.4750 - val_loss: 0.6971 - val_accuracy: 0.4750\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6853 - accuracy: 0.5188 - val_loss: 0.6943 - val_accuracy: 0.4750\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6820 - accuracy: 0.5562 - val_loss: 0.6952 - val_accuracy: 0.4750\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6939 - accuracy: 0.4625 - val_loss: 0.6934 - val_accuracy: 0.4750\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6860 - accuracy: 0.5562 - val_loss: 0.6930 - val_accuracy: 0.5250\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6865 - accuracy: 0.5188 - val_loss: 0.6922 - val_accuracy: 0.5250\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6888 - accuracy: 0.5250 - val_loss: 0.6930 - val_accuracy: 0.4750\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6848 - accuracy: 0.5250 - val_loss: 0.6935 - val_accuracy: 0.4500\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6878 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4500\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6857 - accuracy: 0.4938 - val_loss: 0.6929 - val_accuracy: 0.5500\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6836 - accuracy: 0.5437 - val_loss: 0.6946 - val_accuracy: 0.4750\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6861 - accuracy: 0.4938 - val_loss: 0.6940 - val_accuracy: 0.4750\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6862 - accuracy: 0.5125 - val_loss: 0.6934 - val_accuracy: 0.5250\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6911 - accuracy: 0.4938 - val_loss: 0.6947 - val_accuracy: 0.4250\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6896 - accuracy: 0.4750 - val_loss: 0.6958 - val_accuracy: 0.4750\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 78s 4s/step - loss: 0.6910 - accuracy: 0.4875 - val_loss: 0.6943 - val_accuracy: 0.4500\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 79s 4s/step - loss: 0.6838 - accuracy: 0.5250 - val_loss: 0.6931 - val_accuracy: 0.5250\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 79s 4s/step - loss: 0.6840 - accuracy: 0.4812 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 80s 4s/step - loss: 0.6895 - accuracy: 0.5250 - val_loss: 0.6946 - val_accuracy: 0.4750\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 80s 4s/step - loss: 0.6868 - accuracy: 0.4875 - val_loss: 0.6955 - val_accuracy: 0.4750\n",
            "Final Validation Accuracy for Model 1: 47.50%\n",
            "\n",
            "Training Model 2\n",
            "Epoch 1/50\n",
            "20/20 [==============================] - 123s 6s/step - loss: 0.7558 - accuracy: 0.4625 - val_loss: 0.7396 - val_accuracy: 0.4750\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.7203 - accuracy: 0.5500 - val_loss: 0.7098 - val_accuracy: 0.4750\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.7543 - accuracy: 0.4125 - val_loss: 0.6868 - val_accuracy: 0.5500\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6823 - accuracy: 0.6313 - val_loss: 0.7014 - val_accuracy: 0.5750\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 114s 6s/step - loss: 0.6952 - accuracy: 0.5188 - val_loss: 0.7058 - val_accuracy: 0.4000\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 114s 6s/step - loss: 0.6816 - accuracy: 0.6125 - val_loss: 0.7083 - val_accuracy: 0.4750\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.6966 - accuracy: 0.5562 - val_loss: 0.7070 - val_accuracy: 0.4500\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.7124 - accuracy: 0.5063 - val_loss: 0.7015 - val_accuracy: 0.4500\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.7032 - accuracy: 0.4938 - val_loss: 0.6974 - val_accuracy: 0.4750\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.6786 - accuracy: 0.5625 - val_loss: 0.7011 - val_accuracy: 0.5000\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6802 - accuracy: 0.5500 - val_loss: 0.6989 - val_accuracy: 0.5000\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 115s 6s/step - loss: 0.6955 - accuracy: 0.5188 - val_loss: 0.7026 - val_accuracy: 0.6000\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 114s 6s/step - loss: 0.6678 - accuracy: 0.5938 - val_loss: 0.6892 - val_accuracy: 0.5250\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 114s 6s/step - loss: 0.7035 - accuracy: 0.5188 - val_loss: 0.6944 - val_accuracy: 0.5250\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 114s 6s/step - loss: 0.7111 - accuracy: 0.5312 - val_loss: 0.6990 - val_accuracy: 0.4500\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6920 - accuracy: 0.5500 - val_loss: 0.7072 - val_accuracy: 0.4750\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6988 - accuracy: 0.5000 - val_loss: 0.6992 - val_accuracy: 0.4750\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6978 - accuracy: 0.5063 - val_loss: 0.6988 - val_accuracy: 0.4750\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6929 - accuracy: 0.5250 - val_loss: 0.7055 - val_accuracy: 0.4750\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.7026 - accuracy: 0.5125 - val_loss: 0.7012 - val_accuracy: 0.4750\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6968 - accuracy: 0.4938 - val_loss: 0.6972 - val_accuracy: 0.4500\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 114s 6s/step - loss: 0.6851 - accuracy: 0.5562 - val_loss: 0.7020 - val_accuracy: 0.4750\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6857 - accuracy: 0.5000 - val_loss: 0.7001 - val_accuracy: 0.4750\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6814 - accuracy: 0.5188 - val_loss: 0.6985 - val_accuracy: 0.4750\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.6926 - accuracy: 0.5000 - val_loss: 0.6969 - val_accuracy: 0.6250\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.6840 - accuracy: 0.5500 - val_loss: 0.6979 - val_accuracy: 0.6250\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.6771 - accuracy: 0.5688 - val_loss: 0.6983 - val_accuracy: 0.4750\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.6835 - accuracy: 0.5250 - val_loss: 0.6993 - val_accuracy: 0.4750\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 111s 6s/step - loss: 0.6795 - accuracy: 0.5500 - val_loss: 0.6974 - val_accuracy: 0.6250\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.6841 - accuracy: 0.5312 - val_loss: 0.6985 - val_accuracy: 0.4750\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.6833 - accuracy: 0.5250 - val_loss: 0.7054 - val_accuracy: 0.4750\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6864 - accuracy: 0.5562 - val_loss: 0.6997 - val_accuracy: 0.4750\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6740 - accuracy: 0.6000 - val_loss: 0.7013 - val_accuracy: 0.4750\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 114s 6s/step - loss: 0.6731 - accuracy: 0.6000 - val_loss: 0.7029 - val_accuracy: 0.4750\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 114s 6s/step - loss: 0.6617 - accuracy: 0.5813 - val_loss: 0.7041 - val_accuracy: 0.4750\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.7055 - val_accuracy: 0.4750\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6805 - accuracy: 0.5500 - val_loss: 0.7030 - val_accuracy: 0.4750\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6791 - accuracy: 0.5312 - val_loss: 0.7012 - val_accuracy: 0.6000\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 113s 6s/step - loss: 0.6917 - accuracy: 0.4688 - val_loss: 0.7013 - val_accuracy: 0.6250\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 116s 6s/step - loss: 0.6812 - accuracy: 0.5250 - val_loss: 0.7037 - val_accuracy: 0.4750\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 117s 6s/step - loss: 0.6832 - accuracy: 0.5750 - val_loss: 0.7034 - val_accuracy: 0.4750\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 114s 6s/step - loss: 0.6732 - accuracy: 0.5562 - val_loss: 0.7027 - val_accuracy: 0.4750\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.6887 - accuracy: 0.5250 - val_loss: 0.7036 - val_accuracy: 0.4750\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 111s 6s/step - loss: 0.6739 - accuracy: 0.5625 - val_loss: 0.7086 - val_accuracy: 0.4750\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 112s 6s/step - loss: 0.6775 - accuracy: 0.5688 - val_loss: 0.7076 - val_accuracy: 0.4750\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 115s 6s/step - loss: 0.6835 - accuracy: 0.4875 - val_loss: 0.7042 - val_accuracy: 0.6250\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 120s 6s/step - loss: 0.6844 - accuracy: 0.4875 - val_loss: 0.7076 - val_accuracy: 0.4500\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 122s 6s/step - loss: 0.6793 - accuracy: 0.5500 - val_loss: 0.7069 - val_accuracy: 0.4500\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 122s 6s/step - loss: 0.6737 - accuracy: 0.5250 - val_loss: 0.7091 - val_accuracy: 0.4500\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 123s 6s/step - loss: 0.6895 - accuracy: 0.5500 - val_loss: 0.7082 - val_accuracy: 0.4500\n",
            "Final Validation Accuracy for Model 2: 45.00%\n",
            "\n",
            "Training Model 3\n",
            "Epoch 1/50\n",
            "20/20 [==============================] - 110s 5s/step - loss: 0.7106 - accuracy: 0.5750 - val_loss: 0.6948 - val_accuracy: 0.4750\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 105s 5s/step - loss: 0.7350 - accuracy: 0.5063 - val_loss: 0.6952 - val_accuracy: 0.4750\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 106s 5s/step - loss: 0.7726 - accuracy: 0.4125 - val_loss: 0.7002 - val_accuracy: 0.4750\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.7066 - accuracy: 0.5312 - val_loss: 0.6920 - val_accuracy: 0.5250\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 106s 5s/step - loss: 0.6930 - accuracy: 0.5437 - val_loss: 0.6948 - val_accuracy: 0.4750\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 106s 5s/step - loss: 0.7120 - accuracy: 0.4688 - val_loss: 0.6952 - val_accuracy: 0.4750\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.7155 - accuracy: 0.4750 - val_loss: 0.6927 - val_accuracy: 0.5250\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.7078 - accuracy: 0.4688 - val_loss: 0.6953 - val_accuracy: 0.4750\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.6834 - accuracy: 0.5562 - val_loss: 0.6962 - val_accuracy: 0.4750\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6986 - accuracy: 0.5375 - val_loss: 0.6938 - val_accuracy: 0.5250\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6917 - accuracy: 0.4750 - val_loss: 0.6950 - val_accuracy: 0.4750\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.7043 - accuracy: 0.4938 - val_loss: 0.6943 - val_accuracy: 0.4750\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.7031 - accuracy: 0.4750 - val_loss: 0.6960 - val_accuracy: 0.4750\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6977 - accuracy: 0.4437 - val_loss: 0.6928 - val_accuracy: 0.5250\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.7054 - accuracy: 0.4000 - val_loss: 0.6928 - val_accuracy: 0.5250\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6931 - accuracy: 0.5437 - val_loss: 0.6921 - val_accuracy: 0.5250\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6905 - accuracy: 0.5375 - val_loss: 0.6920 - val_accuracy: 0.4750\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6926 - accuracy: 0.5063 - val_loss: 0.6935 - val_accuracy: 0.4750\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 109s 5s/step - loss: 0.6917 - accuracy: 0.4812 - val_loss: 0.6942 - val_accuracy: 0.4750\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6938 - accuracy: 0.4938 - val_loss: 0.6939 - val_accuracy: 0.4750\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6922 - accuracy: 0.5250 - val_loss: 0.6932 - val_accuracy: 0.5250\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 109s 5s/step - loss: 0.6978 - accuracy: 0.4750 - val_loss: 0.6938 - val_accuracy: 0.4750\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 109s 5s/step - loss: 0.6892 - accuracy: 0.5063 - val_loss: 0.6936 - val_accuracy: 0.5250\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 109s 5s/step - loss: 0.6917 - accuracy: 0.5063 - val_loss: 0.6955 - val_accuracy: 0.4750\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6915 - accuracy: 0.5250 - val_loss: 0.6947 - val_accuracy: 0.4750\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6976 - accuracy: 0.4812 - val_loss: 0.6946 - val_accuracy: 0.4750\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 109s 5s/step - loss: 0.6928 - accuracy: 0.4875 - val_loss: 0.6942 - val_accuracy: 0.5250\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6961 - accuracy: 0.4563 - val_loss: 0.6930 - val_accuracy: 0.5250\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 108s 5s/step - loss: 0.6943 - accuracy: 0.4875 - val_loss: 0.6932 - val_accuracy: 0.5250\n",
            "Epoch 30/50\n",
            " 3/20 [===>..........................] - ETA: 1:26 - loss: 0.6739 - accuracy: 0.6667"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import Xception\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Model 1: CNN-LSTM\n",
        "def build_cnn_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(30, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Model 2: Xception-based CNN-LSTM\n",
        "def build_xception_lstm_model():\n",
        "    base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(base_model, input_shape=(30, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Model 3: Bidirectional-LSTM\n",
        "def build_bidirectional_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(30, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Build and compile models\n",
        "model1 = build_cnn_lstm_model()\n",
        "model2 = build_xception_lstm_model()\n",
        "model3 = build_bidirectional_lstm_model()\n",
        "\n",
        "models = [model1, model2, model3]\n",
        "\n",
        "for model in models:\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load pre-trained weights if available\n",
        "# for i, model in enumerate(models):\n",
        "#     model.load_weights(f'model_{i}_weights.h5')\n",
        "\n",
        "# Frame extraction function\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    \"\"\"Extract frames from the uploaded video and preprocess for model prediction.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "\n",
        "    # Ensure we have the required number of frames by padding if necessary\n",
        "    if len(frames) < max_frames:\n",
        "        print(f\"Warning: {video_path} has only {len(frames)} frames. Padding with black frames.\")\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Add blank frame\n",
        "\n",
        "    return np.array(frames)\n",
        "\n",
        "# Placeholder for training dataset\n",
        "# Replace X_train, X_val, y_train, y_val with actual data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train each model for 10 epochs\n",
        "epochs = 50\n",
        "for i, model in enumerate(models):\n",
        "    print(f\"Training Model {i+1}\")\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=8, validation_data=(X_val, y_val))\n",
        "\n",
        "    # Optionally, save weights after training\n",
        "    model.save_weights(f'model_{i}_weights.h5')\n",
        "\n",
        "    # Print final validation accuracy for each model\n",
        "    val_accuracy = history.history['val_accuracy'][-1]\n",
        "    print(f\"Final Validation Accuracy for Model {i+1}: {val_accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "# Ensemble prediction function\n",
        "def predict_video_ensemble(video_path):\n",
        "    frames = extract_frames(video_path)\n",
        "    frames = frames[np.newaxis, ...]  # Add batch dimension\n",
        "\n",
        "    # Get predictions from each model\n",
        "    predictions = [model.predict(frames)[0][0] for model in models]\n",
        "\n",
        "    # Majority vote or average prediction\n",
        "    avg_prediction = np.mean(predictions)\n",
        "    return \"Fake\" if avg_prediction > 0.5 else \"Real\"\n",
        "\n",
        "# Testing with a video\n",
        "uploaded_video_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id0_0003.mp4'  # Change to your uploaded video's path\n",
        "result = predict_video_ensemble(uploaded_video_path)\n",
        "print(f\"The uploaded video is predicted to be: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx0urEsgjZGq",
        "outputId": "40d2f46c-3fb7-4b8e-b9d1-43925ed572eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 280s 14s/step - loss: 0.8029 - accuracy: 0.4688 - val_loss: 0.7087 - val_accuracy: 0.4000\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 268s 13s/step - loss: 0.7355 - accuracy: 0.5000 - val_loss: 0.7018 - val_accuracy: 0.5250\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 268s 13s/step - loss: 0.7435 - accuracy: 0.5000 - val_loss: 0.6965 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 271s 14s/step - loss: 0.7075 - accuracy: 0.4938 - val_loss: 0.7129 - val_accuracy: 0.4250\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 269s 14s/step - loss: 0.7025 - accuracy: 0.5437 - val_loss: 0.7163 - val_accuracy: 0.4000\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 271s 14s/step - loss: 0.7382 - accuracy: 0.4062 - val_loss: 0.7223 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 273s 14s/step - loss: 0.7182 - accuracy: 0.4875 - val_loss: 0.7075 - val_accuracy: 0.5250\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 275s 14s/step - loss: 0.7223 - accuracy: 0.4500 - val_loss: 0.7083 - val_accuracy: 0.3000\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 277s 14s/step - loss: 0.6907 - accuracy: 0.5188 - val_loss: 0.7005 - val_accuracy: 0.4500\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 277s 14s/step - loss: 0.7071 - accuracy: 0.4313 - val_loss: 0.7050 - val_accuracy: 0.4750\n",
            "Final Validation Accuracy: 47.50%\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "The uploaded video is predicted to be: Fake\n",
            "Warning: /content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id10_0004.mp4  has only 0 frames. Padding with black frames.\n",
            "1/1 [==============================] - 1s 783ms/step\n",
            "The uploaded video is predicted to be: Real\n"
          ]
        }
      ],
      "source": [
        "#combine three diffeent model and run into one set of epoch\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, TimeDistributed, Bidirectional, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import Xception\n",
        "\n",
        "# Frame extraction function\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    \"\"\"Extract frames from the uploaded video and preprocess for model prediction.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "\n",
        "    # Ensure we have the required number of frames by padding if necessary\n",
        "    if len(frames) < max_frames:\n",
        "        print(f\"Warning: {video_path} has only {len(frames)} frames. Padding with black frames.\")\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Add blank frame\n",
        "\n",
        "    return np.array(frames)\n",
        "\n",
        "# Combined ensemble model\n",
        "def build_combined_model():\n",
        "    # Input layer for video frames\n",
        "    input_layer = Input(shape=(30, 224, 224, 3))\n",
        "\n",
        "    # Branch 1: CNN-LSTM\n",
        "    cnn_lstm_branch = TimeDistributed(Conv2D(32, (3, 3), activation='relu'))(input_layer)\n",
        "    cnn_lstm_branch = TimeDistributed(MaxPooling2D((2, 2)))(cnn_lstm_branch)\n",
        "    cnn_lstm_branch = TimeDistributed(Conv2D(64, (3, 3), activation='relu'))(cnn_lstm_branch)\n",
        "    cnn_lstm_branch = TimeDistributed(MaxPooling2D((2, 2)))(cnn_lstm_branch)\n",
        "    cnn_lstm_branch = TimeDistributed(Flatten())(cnn_lstm_branch)\n",
        "    cnn_lstm_branch = LSTM(64)(cnn_lstm_branch)\n",
        "\n",
        "    # Branch 2: Xception-LSTM\n",
        "    xception_base = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    xception_base.trainable = False\n",
        "    xception_branch = TimeDistributed(xception_base)(input_layer)\n",
        "    xception_branch = TimeDistributed(Flatten())(xception_branch)\n",
        "    xception_branch = LSTM(64)(xception_branch)\n",
        "\n",
        "    # Branch 3: Bidirectional-LSTM\n",
        "    bidirectional_branch = TimeDistributed(Conv2D(32, (3, 3), activation='relu'))(input_layer)\n",
        "    bidirectional_branch = TimeDistributed(MaxPooling2D((2, 2)))(bidirectional_branch)\n",
        "    bidirectional_branch = TimeDistributed(Conv2D(64, (3, 3), activation='relu'))(bidirectional_branch)\n",
        "    bidirectional_branch = TimeDistributed(MaxPooling2D((2, 2)))(bidirectional_branch)\n",
        "    bidirectional_branch = TimeDistributed(Flatten())(bidirectional_branch)\n",
        "    bidirectional_branch = Bidirectional(LSTM(64))(bidirectional_branch)\n",
        "\n",
        "    # Concatenate the branches\n",
        "    combined = concatenate([cnn_lstm_branch, xception_branch, bidirectional_branch])\n",
        "\n",
        "    # Fully connected layer for classification\n",
        "    dense_layer = Dense(64, activation='relu')(combined)\n",
        "    dropout_layer = Dropout(0.5)(dense_layer)\n",
        "    output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "    # Model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "# Instantiate and compile the model\n",
        "combined_model = build_combined_model()\n",
        "combined_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Placeholder for training dataset\n",
        "# Replace X_train, X_val, y_train, y_val with actual data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "epochs = 50\n",
        "history = combined_model.fit(X_train, y_train, epochs=epochs, batch_size=8, validation_data=(X_val, y_val))\n",
        "\n",
        "# Print final validation accuracy\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Ensemble prediction function\n",
        "def predict_video(video_path):\n",
        "    frames = extract_frames(video_path)\n",
        "    frames = frames[np.newaxis, ...]  # Add batch dimension\n",
        "    prediction = combined_model.predict(frames)\n",
        "    return \"Fake\" if prediction[0][0] > 0.5 else \"Real\"\n",
        "\n",
        "# Testing with a video\n",
        "uploaded_video_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id0_0003.mp4'  # Change to your uploaded video's path\n",
        "result = predict_video(uploaded_video_path)\n",
        "print(f\"The uploaded video is predicted to be: {result}\")\n",
        "\n",
        "uploaded_video_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id10_0004.mp4 '  # Change to your uploaded video's path\n",
        "result = predict_video(uploaded_video_path)\n",
        "print(f\"The uploaded video is predicted to be: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combine three diffeent model and run into one set of epoch\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, TimeDistributed, Bidirectional, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import Xception\n",
        "\n",
        "# Frame extraction function\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    \"\"\"Extract frames from the uploaded video and preprocess for model prediction.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "\n",
        "    # Ensure we have the required number of frames by padding if necessary\n",
        "    if len(frames) < max_frames:\n",
        "        print(f\"Warning: {video_path} has only {len(frames)} frames. Padding with black frames.\")\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Add blank frame\n",
        "\n",
        "    return np.array(frames)\n",
        "\n",
        "# Combined ensemble model\n",
        "def build_combined_model():\n",
        "    # Input layer for video frames\n",
        "    input_layer = Input(shape=(30, 224, 224, 3))\n",
        "\n",
        "    # Branch 1: CNN-LSTM\n",
        "    cnn_lstm_branch = TimeDistributed(Conv2D(32, (3, 3), activation='relu'))(input_layer)\n",
        "    cnn_lstm_branch = TimeDistributed(MaxPooling2D((2, 2)))(cnn_lstm_branch)\n",
        "    cnn_lstm_branch = TimeDistributed(Conv2D(64, (3, 3), activation='relu'))(cnn_lstm_branch)\n",
        "    cnn_lstm_branch = TimeDistributed(MaxPooling2D((2, 2)))(cnn_lstm_branch)\n",
        "    cnn_lstm_branch = TimeDistributed(Flatten())(cnn_lstm_branch)\n",
        "    cnn_lstm_branch = LSTM(64)(cnn_lstm_branch)\n",
        "\n",
        "    # Branch 2: Xception-LSTM\n",
        "    xception_base = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    xception_base.trainable = False\n",
        "    xception_branch = TimeDistributed(xception_base)(input_layer)\n",
        "    xception_branch = TimeDistributed(Flatten())(xception_branch)\n",
        "    xception_branch = LSTM(64)(xception_branch)\n",
        "\n",
        "    # Branch 3: Bidirectional-LSTM\n",
        "    bidirectional_branch = TimeDistributed(Conv2D(32, (3, 3), activation='relu'))(input_layer)\n",
        "    bidirectional_branch = TimeDistributed(MaxPooling2D((2, 2)))(bidirectional_branch)\n",
        "    bidirectional_branch = TimeDistributed(Conv2D(64, (3, 3), activation='relu'))(bidirectional_branch)\n",
        "    bidirectional_branch = TimeDistributed(MaxPooling2D((2, 2)))(bidirectional_branch)\n",
        "    bidirectional_branch = TimeDistributed(Flatten())(bidirectional_branch)\n",
        "    bidirectional_branch = Bidirectional(LSTM(64))(bidirectional_branch)\n",
        "\n",
        "    # Concatenate the branches\n",
        "    combined = concatenate([cnn_lstm_branch, xception_branch, bidirectional_branch])\n",
        "\n",
        "    # Fully connected layer for classification\n",
        "    dense_layer = Dense(64, activation='relu')(combined)\n",
        "    dropout_layer = Dropout(0.5)(dense_layer)\n",
        "    output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "    # Model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "# Instantiate and compile the model\n",
        "combined_model = build_combined_model()\n",
        "combined_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Placeholder for training dataset\n",
        "# Replace X_train, X_val, y_train, y_val with actual data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "epochs = 50\n",
        "history = combined_model.fit(X_train, y_train, epochs=epochs, batch_size=8, validation_data=(X_val, y_val))\n",
        "\n",
        "# Print final validation accuracy\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Ensemble prediction function\n",
        "def predict_video(video_path):\n",
        "    frames = extract_frames(video_path)\n",
        "    frames = frames[np.newaxis, ...]  # Add batch dimension\n",
        "    prediction = combined_model.predict(frames)\n",
        "    return \"Fake\" if prediction[0][0] > 0.5 else \"Real\"\n",
        "\n",
        "# Testing with a video\n",
        "uploaded_video_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id0_0003.mp4'  # Change to your uploaded video's path\n",
        "result = predict_video(uploaded_video_path)\n",
        "print(f\"The uploaded video is predicted to be: {result}\")\n",
        "\n",
        "uploaded_video_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real/id10_0004.mp4 '  # Change to your uploaded video's path\n",
        "result = predict_video(uploaded_video_path)\n",
        "print(f\"The uploaded video is predicted to be: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxp2RywYem0C",
        "outputId": "cbe12309-a6de-4cbf-b667-a619a3fd615c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 0s 0us/step\n",
            "Epoch 1/50\n",
            "20/20 [==============================] - 283s 14s/step - loss: 0.7458 - accuracy: 0.4375 - val_loss: 0.6916 - val_accuracy: 0.4500\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 274s 14s/step - loss: 0.7125 - accuracy: 0.5000 - val_loss: 0.6961 - val_accuracy: 0.4250\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 275s 14s/step - loss: 0.7078 - accuracy: 0.4688 - val_loss: 0.6935 - val_accuracy: 0.5500\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 275s 14s/step - loss: 0.7043 - accuracy: 0.5500 - val_loss: 0.7011 - val_accuracy: 0.4750\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 276s 14s/step - loss: 0.7201 - accuracy: 0.5063 - val_loss: 0.6902 - val_accuracy: 0.5500\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 276s 14s/step - loss: 0.7056 - accuracy: 0.4688 - val_loss: 0.6925 - val_accuracy: 0.4250\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 277s 14s/step - loss: 0.7044 - accuracy: 0.4625 - val_loss: 0.6932 - val_accuracy: 0.4500\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 277s 14s/step - loss: 0.6915 - accuracy: 0.5375 - val_loss: 0.6912 - val_accuracy: 0.5250\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 278s 14s/step - loss: 0.6947 - accuracy: 0.5500 - val_loss: 0.6923 - val_accuracy: 0.5250\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 278s 14s/step - loss: 0.7194 - accuracy: 0.4250 - val_loss: 0.6939 - val_accuracy: 0.4750\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 279s 14s/step - loss: 0.6958 - accuracy: 0.4875 - val_loss: 0.6969 - val_accuracy: 0.4750\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 278s 14s/step - loss: 0.6876 - accuracy: 0.5375 - val_loss: 0.6938 - val_accuracy: 0.4500\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 277s 14s/step - loss: 0.6949 - accuracy: 0.5188 - val_loss: 0.6935 - val_accuracy: 0.4750\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 277s 14s/step - loss: 0.6926 - accuracy: 0.5688 - val_loss: 0.6935 - val_accuracy: 0.4750\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 277s 14s/step - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5500\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 278s 14s/step - loss: 0.6921 - accuracy: 0.5250 - val_loss: 0.6956 - val_accuracy: 0.5000\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 279s 14s/step - loss: 0.6921 - accuracy: 0.5063 - val_loss: 0.6969 - val_accuracy: 0.4000\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.6724 - accuracy: 0.6062 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tranfer learning\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, TimeDistributed, LSTM, Dropout\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Paths to datasets\n",
        "real_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real'\n",
        "fake_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-synthesis'\n",
        "\n",
        "# Function to extract frames from videos\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    if len(frames) < max_frames:\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Padding if frames < max_frames\n",
        "    return np.array(frames)\n",
        "\n",
        "# Load data\n",
        "def load_data(data_dir, label, max_videos=100):\n",
        "    videos, labels = [], []\n",
        "    for i, filename in enumerate(os.listdir(data_dir)):\n",
        "        if i == max_videos:\n",
        "            break\n",
        "        filepath = os.path.join(data_dir, filename)\n",
        "        frames = extract_frames(filepath)\n",
        "        if len(frames) == 30:  # Check that each video has 30 frames\n",
        "            videos.append(frames)\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipped {filename}: Not enough frames or corrupted video.\")\n",
        "    return np.array(videos), np.array(labels)\n",
        "\n",
        "# Load real and fake video datasets\n",
        "real_videos, real_labels = load_data(real_videos_path, label=0)\n",
        "fake_videos, fake_labels = load_data(fake_videos_path, label=1)\n",
        "\n",
        "# Combine and shuffle the dataset\n",
        "X = np.concatenate((real_videos, fake_videos))\n",
        "y = np.concatenate((real_labels, fake_labels))\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build transfer learning model with Xception as base\n",
        "def build_transfer_learning_model():\n",
        "    base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False  # Freeze the base model layers\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(base_model, input_shape=(30, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_transfer_learning_model()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate model performance\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ATTUJIKW5KXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7eefd0-7e93-4ea1-9b20-c5e6a30c1b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.7409 - accuracy: 0.4938 - val_loss: 0.7115 - val_accuracy: 0.4750\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 94s 5s/step - loss: 0.6699 - accuracy: 0.5938 - val_loss: 0.7276 - val_accuracy: 0.4750\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 94s 5s/step - loss: 0.6958 - accuracy: 0.5250 - val_loss: 0.7157 - val_accuracy: 0.5500\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 94s 5s/step - loss: 0.7005 - accuracy: 0.5312 - val_loss: 0.7263 - val_accuracy: 0.4500\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 95s 5s/step - loss: 0.6759 - accuracy: 0.5813 - val_loss: 0.7078 - val_accuracy: 0.6000\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 98s 5s/step - loss: 0.6517 - accuracy: 0.5875 - val_loss: 0.7267 - val_accuracy: 0.5750\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.6770 - accuracy: 0.5750 - val_loss: 0.7215 - val_accuracy: 0.5750\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6487 - accuracy: 0.5938 - val_loss: 0.7581 - val_accuracy: 0.4750\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6309 - accuracy: 0.6438 - val_loss: 0.7508 - val_accuracy: 0.4750\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 103s 5s/step - loss: 0.6369 - accuracy: 0.5875 - val_loss: 0.7944 - val_accuracy: 0.3750\n",
            "Final Training Accuracy: 58.75%\n",
            "Final Validation Accuracy: 37.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tranfer learning\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, TimeDistributed, LSTM, Dropout\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Paths to datasets\n",
        "real_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real'\n",
        "fake_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-synthesis'\n",
        "\n",
        "# Function to extract frames from videos\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    if len(frames) < max_frames:\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Padding if frames < max_frames\n",
        "    return np.array(frames)\n",
        "\n",
        "# Load data\n",
        "def load_data(data_dir, label, max_videos=100):\n",
        "    videos, labels = [], []\n",
        "    for i, filename in enumerate(os.listdir(data_dir)):\n",
        "        if i == max_videos:\n",
        "            break\n",
        "        filepath = os.path.join(data_dir, filename)\n",
        "        frames = extract_frames(filepath)\n",
        "        if len(frames) == 30:  # Check that each video has 30 frames\n",
        "            videos.append(frames)\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipped {filename}: Not enough frames or corrupted video.\")\n",
        "    return np.array(videos), np.array(labels)\n",
        "\n",
        "# Load real and fake video datasets\n",
        "real_videos, real_labels = load_data(real_videos_path, label=0)\n",
        "fake_videos, fake_labels = load_data(fake_videos_path, label=1)\n",
        "\n",
        "# Combine and shuffle the dataset\n",
        "X = np.concatenate((real_videos, fake_videos))\n",
        "y = np.concatenate((real_labels, fake_labels))\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build transfer learning model with Xception as base\n",
        "def build_transfer_learning_model():\n",
        "    base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False  # Freeze the base model layers\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(base_model, input_shape=(30, 224, 224, 3)))\n",
        "    model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_transfer_learning_model()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate model performance\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nx6u4hac0LX",
        "outputId": "c3a61c03-f62c-46c7-d669-581114680fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 0s 0us/step\n",
            "Epoch 1/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.7665 - accuracy: 0.5000 - val_loss: 0.7190 - val_accuracy: 0.4250\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 94s 5s/step - loss: 0.7005 - accuracy: 0.5437 - val_loss: 0.7109 - val_accuracy: 0.3250\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 95s 5s/step - loss: 0.6869 - accuracy: 0.5875 - val_loss: 0.7205 - val_accuracy: 0.5250\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 95s 5s/step - loss: 0.6922 - accuracy: 0.5813 - val_loss: 0.7249 - val_accuracy: 0.4750\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 96s 5s/step - loss: 0.6807 - accuracy: 0.5688 - val_loss: 0.7250 - val_accuracy: 0.4500\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 97s 5s/step - loss: 0.7031 - accuracy: 0.4938 - val_loss: 0.7367 - val_accuracy: 0.4000\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.6580 - accuracy: 0.6250 - val_loss: 0.7380 - val_accuracy: 0.3500\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 98s 5s/step - loss: 0.6685 - accuracy: 0.5813 - val_loss: 0.7377 - val_accuracy: 0.4750\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.6457 - accuracy: 0.6062 - val_loss: 0.7491 - val_accuracy: 0.5500\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.6196 - accuracy: 0.6313 - val_loss: 0.7756 - val_accuracy: 0.4250\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.5962 - accuracy: 0.6625 - val_loss: 0.7961 - val_accuracy: 0.4750\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.6003 - accuracy: 0.6438 - val_loss: 0.7760 - val_accuracy: 0.5000\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.6153 - accuracy: 0.6062 - val_loss: 0.8147 - val_accuracy: 0.5500\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.6024 - accuracy: 0.6562 - val_loss: 0.8244 - val_accuracy: 0.4000\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.6011 - accuracy: 0.6438 - val_loss: 0.8915 - val_accuracy: 0.3750\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.5927 - accuracy: 0.6438 - val_loss: 0.8657 - val_accuracy: 0.5250\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.6080 - accuracy: 0.6000 - val_loss: 0.9191 - val_accuracy: 0.3750\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.5786 - accuracy: 0.6500 - val_loss: 0.9384 - val_accuracy: 0.4500\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 98s 5s/step - loss: 0.5565 - accuracy: 0.6812 - val_loss: 0.9561 - val_accuracy: 0.3500\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.5787 - accuracy: 0.6500 - val_loss: 0.8854 - val_accuracy: 0.4250\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.5738 - accuracy: 0.6250 - val_loss: 0.9804 - val_accuracy: 0.4000\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.5733 - accuracy: 0.6562 - val_loss: 0.9647 - val_accuracy: 0.4000\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.5461 - accuracy: 0.7250 - val_loss: 0.9967 - val_accuracy: 0.3500\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.5356 - accuracy: 0.6812 - val_loss: 0.9430 - val_accuracy: 0.3750\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.5186 - accuracy: 0.7188 - val_loss: 0.9865 - val_accuracy: 0.4250\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.5354 - accuracy: 0.6812 - val_loss: 1.0666 - val_accuracy: 0.4500\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.5440 - accuracy: 0.6687 - val_loss: 1.0280 - val_accuracy: 0.5000\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.5156 - accuracy: 0.7375 - val_loss: 1.0808 - val_accuracy: 0.3750\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.5493 - accuracy: 0.6687 - val_loss: 1.0091 - val_accuracy: 0.4250\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.5686 - accuracy: 0.6500 - val_loss: 0.9487 - val_accuracy: 0.4500\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 98s 5s/step - loss: 0.5498 - accuracy: 0.6938 - val_loss: 0.9254 - val_accuracy: 0.4750\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.5900 - accuracy: 0.6500 - val_loss: 0.9809 - val_accuracy: 0.4250\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.5297 - accuracy: 0.6875 - val_loss: 1.0151 - val_accuracy: 0.3750\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.5462 - accuracy: 0.6313 - val_loss: 1.0405 - val_accuracy: 0.4500\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.5019 - accuracy: 0.7437 - val_loss: 1.0401 - val_accuracy: 0.4750\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.4945 - accuracy: 0.7625 - val_loss: 1.0510 - val_accuracy: 0.4000\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.5437 - accuracy: 0.6875 - val_loss: 1.0471 - val_accuracy: 0.4750\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.4969 - accuracy: 0.7188 - val_loss: 1.0809 - val_accuracy: 0.4750\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.5192 - accuracy: 0.7063 - val_loss: 1.0814 - val_accuracy: 0.4500\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.4998 - accuracy: 0.7000 - val_loss: 1.0834 - val_accuracy: 0.5250\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 99s 5s/step - loss: 0.5189 - accuracy: 0.6625 - val_loss: 1.1482 - val_accuracy: 0.4250\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.5113 - accuracy: 0.6750 - val_loss: 1.1104 - val_accuracy: 0.4250\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.4965 - accuracy: 0.7000 - val_loss: 1.1151 - val_accuracy: 0.4500\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.4910 - accuracy: 0.7312 - val_loss: 1.1352 - val_accuracy: 0.4500\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 103s 5s/step - loss: 0.4994 - accuracy: 0.6875 - val_loss: 1.1527 - val_accuracy: 0.4250\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 104s 5s/step - loss: 0.4643 - accuracy: 0.7625 - val_loss: 1.1461 - val_accuracy: 0.4250\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.4872 - accuracy: 0.7312 - val_loss: 1.1881 - val_accuracy: 0.4250\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.4973 - accuracy: 0.7000 - val_loss: 1.1908 - val_accuracy: 0.4000\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.5175 - accuracy: 0.6812 - val_loss: 1.1681 - val_accuracy: 0.5000\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 107s 5s/step - loss: 0.4873 - accuracy: 0.7063 - val_loss: 1.1664 - val_accuracy: 0.4750\n",
            "Final Training Accuracy: 70.63%\n",
            "Final Validation Accuracy: 47.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, TimeDistributed, LSTM, Dropout, Input, Concatenate\n",
        "from tensorflow.keras.applications import Xception, VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Paths to datasets\n",
        "real_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real'\n",
        "fake_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-synthesis'\n",
        "\n",
        "# Function to extract frames from videos\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    if len(frames) < max_frames:\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Padding if frames < max_frames\n",
        "    return np.array(frames)\n",
        "\n",
        "# Load data\n",
        "def load_data(data_dir, label, max_videos=100):\n",
        "    videos, labels = [], []\n",
        "    for i, filename in enumerate(os.listdir(data_dir)):\n",
        "        if i == max_videos:\n",
        "            break\n",
        "        filepath = os.path.join(data_dir, filename)\n",
        "        frames = extract_frames(filepath)\n",
        "        if len(frames) == 30:  # Check that each video has 30 frames\n",
        "            videos.append(frames)\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipped {filename}: Not enough frames or corrupted video.\")\n",
        "    return np.array(videos), np.array(labels)\n",
        "\n",
        "# Load real and fake video datasets\n",
        "real_videos, real_labels = load_data(real_videos_path, label=0)\n",
        "fake_videos, fake_labels = load_data(fake_videos_path, label=1)\n",
        "\n",
        "# Combine and shuffle the dataset\n",
        "X = np.concatenate((real_videos, fake_videos))\n",
        "y = np.concatenate((real_labels, fake_labels))\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build transfer learning model with Xception and VGG16 as base\n",
        "def build_transfer_learning_model():\n",
        "    # Define a single input layer\n",
        "    input_layer = Input(shape=(30, 224, 224, 3))\n",
        "\n",
        "    # Xception branch\n",
        "    xception_base = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    xception_base.trainable = False\n",
        "    xception_branch = TimeDistributed(xception_base)(input_layer)\n",
        "    xception_branch = TimeDistributed(GlobalAveragePooling2D())(xception_branch)\n",
        "\n",
        "    # VGG16 branch\n",
        "    vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    vgg_base.trainable = False\n",
        "    vgg_branch = TimeDistributed(vgg_base)(input_layer)\n",
        "    vgg_branch = TimeDistributed(GlobalAveragePooling2D())(vgg_branch)\n",
        "\n",
        "    # Concatenate outputs of both branches\n",
        "    concatenated = Concatenate()([xception_branch, vgg_branch])\n",
        "\n",
        "    # LSTM and Dense layers\n",
        "    x = LSTM(64)(concatenated)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Create the model with input_layer as the input\n",
        "    model = Model(inputs=input_layer, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_transfer_learning_model()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate model performance\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dT88rDselTN",
        "outputId": "879c5f4b-074e-4fab-f508-dc086fda1ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.7339 - accuracy: 0.5437 - val_loss: 0.7017 - val_accuracy: 0.5750\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 176s 9s/step - loss: 0.7246 - accuracy: 0.4750 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 178s 9s/step - loss: 0.6911 - accuracy: 0.5312 - val_loss: 0.7055 - val_accuracy: 0.4750\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 178s 9s/step - loss: 0.6785 - accuracy: 0.6000 - val_loss: 0.7182 - val_accuracy: 0.4000\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 178s 9s/step - loss: 0.6740 - accuracy: 0.5688 - val_loss: 0.7200 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 180s 9s/step - loss: 0.6358 - accuracy: 0.6500 - val_loss: 0.7109 - val_accuracy: 0.5500\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 181s 9s/step - loss: 0.6457 - accuracy: 0.6000 - val_loss: 0.7257 - val_accuracy: 0.5250\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.6555 - accuracy: 0.5500 - val_loss: 0.7397 - val_accuracy: 0.4000\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 183s 9s/step - loss: 0.6627 - accuracy: 0.6250 - val_loss: 0.7562 - val_accuracy: 0.3500\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 184s 9s/step - loss: 0.6431 - accuracy: 0.5875 - val_loss: 0.7675 - val_accuracy: 0.4000\n",
            "Final Training Accuracy: 58.75%\n",
            "Final Validation Accuracy: 40.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, TimeDistributed, LSTM, Dropout, Input, Concatenate\n",
        "from tensorflow.keras.applications import Xception, VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Paths to datasets\n",
        "real_videos_path = '/content/drive/MyDrive/Celeb-DF/Celeb-real'\n",
        "fake_videos_path = '/content/drive/MyDrive/Celeb-DF/Celeb-synthesis'\n",
        "\n",
        "# Function to extract frames from videos\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    if len(frames) < max_frames:\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Padding if frames < max_frames\n",
        "    return np.array(frames)\n",
        "\n",
        "# Load data\n",
        "def load_data(data_dir, label, max_videos=100):\n",
        "    videos, labels = [], []\n",
        "    for i, filename in enumerate(os.listdir(data_dir)):\n",
        "        if i == max_videos:\n",
        "            break\n",
        "        filepath = os.path.join(data_dir, filename)\n",
        "        frames = extract_frames(filepath)\n",
        "        if len(frames) == 30:  # Check that each video has 30 frames\n",
        "            videos.append(frames)\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipped {filename}: Not enough frames or corrupted video.\")\n",
        "    return np.array(videos), np.array(labels)\n",
        "\n",
        "# Load real and fake video datasets\n",
        "real_videos, real_labels = load_data(real_videos_path, label=0)\n",
        "fake_videos, fake_labels = load_data(fake_videos_path, label=1)\n",
        "\n",
        "# Combine and shuffle the dataset\n",
        "X = np.concatenate((real_videos, fake_videos))\n",
        "y = np.concatenate((real_labels, fake_labels))\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build transfer learning model with Xception and VGG16 as base\n",
        "def build_transfer_learning_model():\n",
        "    # Define a single input layer\n",
        "    input_layer = Input(shape=(30, 224, 224, 3))\n",
        "\n",
        "    # Xception branch\n",
        "    xception_base = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    xception_base.trainable = False\n",
        "    xception_branch = TimeDistributed(xception_base)(input_layer)\n",
        "    xception_branch = TimeDistributed(GlobalAveragePooling2D())(xception_branch)\n",
        "\n",
        "    # VGG16 branch\n",
        "    vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    vgg_base.trainable = False\n",
        "    vgg_branch = TimeDistributed(vgg_base)(input_layer)\n",
        "    vgg_branch = TimeDistributed(GlobalAveragePooling2D())(vgg_branch)\n",
        "\n",
        "    # Concatenate outputs of both branches\n",
        "    concatenated = Concatenate()([xception_branch, vgg_branch])\n",
        "\n",
        "    # LSTM and Dense layers\n",
        "    x = LSTM(64)(concatenated)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Create the model with input_layer as the input\n",
        "    model = Model(inputs=input_layer, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_transfer_learning_model()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate model performance\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTHyfr93nOz9",
        "outputId": "4a692c95-2218-4b1c-945d-4609be1fcfcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/50\n",
            "20/20 [==============================] - 181s 9s/step - loss: 0.7229 - accuracy: 0.5250 - val_loss: 0.7012 - val_accuracy: 0.4500\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 175s 9s/step - loss: 0.6797 - accuracy: 0.5875 - val_loss: 0.7119 - val_accuracy: 0.3750\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 176s 9s/step - loss: 0.6834 - accuracy: 0.5562 - val_loss: 0.7070 - val_accuracy: 0.4750\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 176s 9s/step - loss: 0.6756 - accuracy: 0.5688 - val_loss: 0.7089 - val_accuracy: 0.4500\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 177s 9s/step - loss: 0.6848 - accuracy: 0.5500 - val_loss: 0.7303 - val_accuracy: 0.4500\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 177s 9s/step - loss: 0.6525 - accuracy: 0.5875 - val_loss: 0.7299 - val_accuracy: 0.4500\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 178s 9s/step - loss: 0.6534 - accuracy: 0.5750 - val_loss: 0.7148 - val_accuracy: 0.5750\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 179s 9s/step - loss: 0.6407 - accuracy: 0.6687 - val_loss: 0.7106 - val_accuracy: 0.5500\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 180s 9s/step - loss: 0.5937 - accuracy: 0.6687 - val_loss: 0.7240 - val_accuracy: 0.5500\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 179s 9s/step - loss: 0.6024 - accuracy: 0.6687 - val_loss: 0.7570 - val_accuracy: 0.4250\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 180s 9s/step - loss: 0.5861 - accuracy: 0.6875 - val_loss: 0.7836 - val_accuracy: 0.4500\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 180s 9s/step - loss: 0.6206 - accuracy: 0.6500 - val_loss: 0.7326 - val_accuracy: 0.4250\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 181s 9s/step - loss: 0.6274 - accuracy: 0.5813 - val_loss: 0.7368 - val_accuracy: 0.4250\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 181s 9s/step - loss: 0.6177 - accuracy: 0.6438 - val_loss: 0.7586 - val_accuracy: 0.4500\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5943 - accuracy: 0.6625 - val_loss: 0.7887 - val_accuracy: 0.5250\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5948 - accuracy: 0.6438 - val_loss: 0.7455 - val_accuracy: 0.5500\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5392 - accuracy: 0.7188 - val_loss: 0.7446 - val_accuracy: 0.6000\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 183s 9s/step - loss: 0.5550 - accuracy: 0.6687 - val_loss: 0.8224 - val_accuracy: 0.5000\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 183s 9s/step - loss: 0.5331 - accuracy: 0.7250 - val_loss: 0.8189 - val_accuracy: 0.4750\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5787 - accuracy: 0.6562 - val_loss: 0.8515 - val_accuracy: 0.5250\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5235 - accuracy: 0.7188 - val_loss: 0.8313 - val_accuracy: 0.4500\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 181s 9s/step - loss: 0.5441 - accuracy: 0.7000 - val_loss: 0.8570 - val_accuracy: 0.4250\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 181s 9s/step - loss: 0.5573 - accuracy: 0.6625 - val_loss: 0.8523 - val_accuracy: 0.4750\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 181s 9s/step - loss: 0.5575 - accuracy: 0.6875 - val_loss: 0.9002 - val_accuracy: 0.5250\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 183s 9s/step - loss: 0.5333 - accuracy: 0.6687 - val_loss: 0.8604 - val_accuracy: 0.4750\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 184s 9s/step - loss: 0.5561 - accuracy: 0.6938 - val_loss: 0.8524 - val_accuracy: 0.5250\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 183s 9s/step - loss: 0.5221 - accuracy: 0.7500 - val_loss: 0.8901 - val_accuracy: 0.5250\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5291 - accuracy: 0.7125 - val_loss: 0.9428 - val_accuracy: 0.4500\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5732 - accuracy: 0.6687 - val_loss: 0.8752 - val_accuracy: 0.5750\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 181s 9s/step - loss: 0.5120 - accuracy: 0.7000 - val_loss: 0.9207 - val_accuracy: 0.4750\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5233 - accuracy: 0.7125 - val_loss: 0.9378 - val_accuracy: 0.5750\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5180 - accuracy: 0.7312 - val_loss: 0.9318 - val_accuracy: 0.5500\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 181s 9s/step - loss: 0.4928 - accuracy: 0.7437 - val_loss: 1.0256 - val_accuracy: 0.5750\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5432 - accuracy: 0.6812 - val_loss: 0.8767 - val_accuracy: 0.5750\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5457 - accuracy: 0.7250 - val_loss: 0.9303 - val_accuracy: 0.3750\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5475 - accuracy: 0.6812 - val_loss: 0.9190 - val_accuracy: 0.3750\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5031 - accuracy: 0.6875 - val_loss: 0.9685 - val_accuracy: 0.3750\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5221 - accuracy: 0.6687 - val_loss: 0.9180 - val_accuracy: 0.4250\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5475 - accuracy: 0.6687 - val_loss: 0.8697 - val_accuracy: 0.5500\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 183s 9s/step - loss: 0.5350 - accuracy: 0.6625 - val_loss: 0.8797 - val_accuracy: 0.6000\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 182s 9s/step - loss: 0.5068 - accuracy: 0.7063 - val_loss: 0.8991 - val_accuracy: 0.5500\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 183s 9s/step - loss: 0.5560 - accuracy: 0.6375 - val_loss: 0.9401 - val_accuracy: 0.5500\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 184s 9s/step - loss: 0.5280 - accuracy: 0.7063 - val_loss: 0.9451 - val_accuracy: 0.5500\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 183s 9s/step - loss: 0.5002 - accuracy: 0.7125 - val_loss: 0.8708 - val_accuracy: 0.5000\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 181s 9s/step - loss: 0.5291 - accuracy: 0.7250 - val_loss: 0.9290 - val_accuracy: 0.4500\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 183s 9s/step - loss: 0.5123 - accuracy: 0.7125 - val_loss: 0.9657 - val_accuracy: 0.5000\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 185s 9s/step - loss: 0.4710 - accuracy: 0.7312 - val_loss: 0.9601 - val_accuracy: 0.5000\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 186s 9s/step - loss: 0.4996 - accuracy: 0.7188 - val_loss: 0.9860 - val_accuracy: 0.4750\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 187s 9s/step - loss: 0.4432 - accuracy: 0.7688 - val_loss: 1.1188 - val_accuracy: 0.4250\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 188s 9s/step - loss: 0.4826 - accuracy: 0.7625 - val_loss: 1.0711 - val_accuracy: 0.4250\n",
            "Final Training Accuracy: 76.25%\n",
            "Final Validation Accuracy: 42.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, GlobalAveragePooling2D, TimeDistributed, LSTM, Dropout, Input, Concatenate, Attention, Reshape,Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Paths to datasets\n",
        "real_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real'\n",
        "fake_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-synthesis'\n",
        "\n",
        "# Function to extract frames from videos\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    if len(frames) < max_frames:\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Padding if frames < max_frames\n",
        "    return np.array(frames)\n",
        "\n",
        "# Load data\n",
        "def load_data(data_dir, label, max_videos=100):\n",
        "    videos, labels = [], []\n",
        "    for i, filename in enumerate(os.listdir(data_dir)):\n",
        "        if i == max_videos:\n",
        "            break\n",
        "        filepath = os.path.join(data_dir, filename)\n",
        "        frames = extract_frames(filepath)\n",
        "        if len(frames) == 30:  # Check that each video has 30 frames\n",
        "            videos.append(frames)\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipped {filename}: Not enough frames or corrupted video.\")\n",
        "    return np.array(videos), np.array(labels)\n",
        "\n",
        "# Load real and fake video datasets\n",
        "real_videos, real_labels = load_data(real_videos_path, label=0)\n",
        "fake_videos, fake_labels = load_data(fake_videos_path, label=1)\n",
        "\n",
        "# Combine and shuffle the dataset\n",
        "X = np.concatenate((real_videos, fake_videos))\n",
        "y = np.concatenate((real_labels, fake_labels))\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model with CNN and Attention Mechanism\n",
        "def build_attention_cnn_model():\n",
        "    input_layer = Input(shape=(30, 224, 224, 3))  # 30 frames, 224x224 resolution, 3 channels\n",
        "\n",
        "    # CNN feature extractor for each frame\n",
        "    cnn = TimeDistributed(Conv2D(32, (3, 3), activation='relu'))(input_layer)\n",
        "    cnn = TimeDistributed(Conv2D(64, (3, 3), activation='relu'))(cnn)\n",
        "    cnn = TimeDistributed(GlobalAveragePooling2D())(cnn)  # Pooling to reduce spatial dimensions\n",
        "\n",
        "    # Apply Attention mechanism\n",
        "    attention = Attention()([cnn, cnn])  # Self-attention (query and value are the same)\n",
        "\n",
        "    # Flatten the attention output to feed into LSTM\n",
        "    attention = TimeDistributed(Flatten())(attention)\n",
        "\n",
        "    # Apply LSTM to capture temporal dependencies between frames\n",
        "    x = LSTM(64)(attention)\n",
        "\n",
        "    # Dense and Dropout layers for classification\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer for binary classification\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model(inputs=input_layer, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_attention_cnn_model()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate model performance\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPt5ioSnnBjq",
        "outputId": "0331c93d-9f06-4588-f3ff-c60207530a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 104s 5s/step - loss: 0.7229 - accuracy: 0.4938 - val_loss: 0.7123 - val_accuracy: 0.4500\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 100s 5s/step - loss: 0.6882 - accuracy: 0.4875 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.7106 - accuracy: 0.4688 - val_loss: 0.6950 - val_accuracy: 0.4500\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.6834 - accuracy: 0.5375 - val_loss: 0.6950 - val_accuracy: 0.4500\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.7044 - accuracy: 0.4688 - val_loss: 0.6932 - val_accuracy: 0.4500\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.7043 - accuracy: 0.4688 - val_loss: 0.6951 - val_accuracy: 0.4500\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7054 - accuracy: 0.4875 - val_loss: 0.7016 - val_accuracy: 0.4500\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7069 - accuracy: 0.4875 - val_loss: 0.6931 - val_accuracy: 0.3250\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6951 - accuracy: 0.4875 - val_loss: 0.6937 - val_accuracy: 0.4500\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7037 - accuracy: 0.4812 - val_loss: 0.6977 - val_accuracy: 0.4500\n",
            "Final Training Accuracy: 48.12%\n",
            "Final Validation Accuracy: 45.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, GlobalAveragePooling2D, TimeDistributed, LSTM, Dropout, Input, Concatenate, Attention, Reshape,Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Paths to datasets\n",
        "real_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real'\n",
        "fake_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-synthesis'\n",
        "\n",
        "# Function to extract frames from videos\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    if len(frames) < max_frames:\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Padding if frames < max_frames\n",
        "    return np.array(frames)\n",
        "\n",
        "# Load data\n",
        "def load_data(data_dir, label, max_videos=100):\n",
        "    videos, labels = [], []\n",
        "    for i, filename in enumerate(os.listdir(data_dir)):\n",
        "        if i == max_videos:\n",
        "            break\n",
        "        filepath = os.path.join(data_dir, filename)\n",
        "        frames = extract_frames(filepath)\n",
        "        if len(frames) == 30:  # Check that each video has 30 frames\n",
        "            videos.append(frames)\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipped {filename}: Not enough frames or corrupted video.\")\n",
        "    return np.array(videos), np.array(labels)\n",
        "\n",
        "# Load real and fake video datasets\n",
        "real_videos, real_labels = load_data(real_videos_path, label=0)\n",
        "fake_videos, fake_labels = load_data(fake_videos_path, label=1)\n",
        "\n",
        "# Combine and shuffle the dataset\n",
        "X = np.concatenate((real_videos, fake_videos))\n",
        "y = np.concatenate((real_labels, fake_labels))\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model with CNN and Attention Mechanism\n",
        "def build_attention_cnn_model():\n",
        "    input_layer = Input(shape=(30, 224, 224, 3))  # 30 frames, 224x224 resolution, 3 channels\n",
        "\n",
        "    # CNN feature extractor for each frame\n",
        "    cnn = TimeDistributed(Conv2D(32, (3, 3), activation='relu'))(input_layer)\n",
        "    cnn = TimeDistributed(Conv2D(64, (3, 3), activation='relu'))(cnn)\n",
        "    cnn = TimeDistributed(GlobalAveragePooling2D())(cnn)  # Pooling to reduce spatial dimensions\n",
        "\n",
        "    # Apply Attention mechanism\n",
        "    attention = Attention()([cnn, cnn])  # Self-attention (query and value are the same)\n",
        "\n",
        "    # Flatten the attention output to feed into LSTM\n",
        "    attention = TimeDistributed(Flatten())(attention)\n",
        "\n",
        "    # Apply LSTM to capture temporal dependencies between frames\n",
        "    x = LSTM(64)(attention)\n",
        "\n",
        "    # Dense and Dropout layers for classification\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer for binary classification\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model(inputs=input_layer, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_attention_cnn_model()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate model performance\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeNu5GxAnBnG",
        "outputId": "cc5df35d-2946-4905-dbf0-9e38a8613bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "20/20 [==============================] - 104s 5s/step - loss: 0.7118 - accuracy: 0.5188 - val_loss: 0.7172 - val_accuracy: 0.4500\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.7010 - accuracy: 0.5000 - val_loss: 0.7140 - val_accuracy: 0.3500\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.7258 - accuracy: 0.5188 - val_loss: 0.7067 - val_accuracy: 0.4500\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7269 - accuracy: 0.4437 - val_loss: 0.7128 - val_accuracy: 0.4500\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6878 - accuracy: 0.5375 - val_loss: 0.7016 - val_accuracy: 0.4500\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.6979 - accuracy: 0.5063 - val_loss: 0.6922 - val_accuracy: 0.5500\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.7063 - accuracy: 0.4688 - val_loss: 0.6956 - val_accuracy: 0.4500\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 101s 5s/step - loss: 0.7014 - accuracy: 0.5562 - val_loss: 0.6978 - val_accuracy: 0.4500\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6970 - accuracy: 0.5063 - val_loss: 0.6991 - val_accuracy: 0.4500\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7024 - accuracy: 0.5500 - val_loss: 0.6962 - val_accuracy: 0.4500\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7202 - accuracy: 0.4625 - val_loss: 0.6961 - val_accuracy: 0.4500\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7210 - accuracy: 0.4062 - val_loss: 0.6992 - val_accuracy: 0.4500\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7137 - accuracy: 0.4812 - val_loss: 0.6952 - val_accuracy: 0.4500\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6952 - accuracy: 0.5063 - val_loss: 0.6970 - val_accuracy: 0.4500\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7072 - accuracy: 0.5000 - val_loss: 0.6987 - val_accuracy: 0.4500\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6946 - accuracy: 0.5312 - val_loss: 0.7051 - val_accuracy: 0.4500\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7105 - accuracy: 0.4125 - val_loss: 0.6934 - val_accuracy: 0.4250\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7019 - accuracy: 0.4750 - val_loss: 0.6944 - val_accuracy: 0.4500\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7077 - accuracy: 0.4750 - val_loss: 0.6964 - val_accuracy: 0.4500\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6962 - accuracy: 0.4563 - val_loss: 0.6981 - val_accuracy: 0.4500\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7103 - accuracy: 0.3938 - val_loss: 0.6930 - val_accuracy: 0.5750\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6924 - accuracy: 0.4938 - val_loss: 0.6954 - val_accuracy: 0.4500\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6936 - accuracy: 0.5188 - val_loss: 0.6917 - val_accuracy: 0.5500\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6926 - accuracy: 0.5500 - val_loss: 0.6961 - val_accuracy: 0.4500\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6831 - accuracy: 0.5437 - val_loss: 0.6940 - val_accuracy: 0.4500\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7008 - accuracy: 0.4500 - val_loss: 0.6936 - val_accuracy: 0.4500\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6901 - accuracy: 0.5562 - val_loss: 0.7019 - val_accuracy: 0.4500\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6986 - accuracy: 0.5000 - val_loss: 0.7005 - val_accuracy: 0.4250\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7021 - accuracy: 0.4688 - val_loss: 0.7073 - val_accuracy: 0.4500\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6960 - accuracy: 0.4812 - val_loss: 0.6941 - val_accuracy: 0.4500\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7111 - accuracy: 0.4750 - val_loss: 0.6977 - val_accuracy: 0.4500\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6880 - accuracy: 0.5437 - val_loss: 0.7005 - val_accuracy: 0.4500\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6974 - accuracy: 0.5250 - val_loss: 0.6955 - val_accuracy: 0.4500\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6966 - accuracy: 0.5000 - val_loss: 0.6988 - val_accuracy: 0.4500\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7017 - accuracy: 0.4938 - val_loss: 0.6936 - val_accuracy: 0.4500\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6898 - accuracy: 0.5375 - val_loss: 0.6982 - val_accuracy: 0.4500\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6958 - accuracy: 0.5437 - val_loss: 0.6919 - val_accuracy: 0.5500\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6934 - accuracy: 0.5750 - val_loss: 0.6940 - val_accuracy: 0.4500\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7021 - accuracy: 0.4625 - val_loss: 0.6969 - val_accuracy: 0.4500\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6932 - accuracy: 0.5125 - val_loss: 0.6917 - val_accuracy: 0.5500\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6947 - accuracy: 0.5188 - val_loss: 0.6958 - val_accuracy: 0.4500\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7017 - accuracy: 0.5063 - val_loss: 0.6959 - val_accuracy: 0.4500\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 103s 5s/step - loss: 0.6933 - accuracy: 0.5125 - val_loss: 0.6938 - val_accuracy: 0.4500\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 103s 5s/step - loss: 0.6950 - accuracy: 0.4938 - val_loss: 0.6935 - val_accuracy: 0.4500\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.7013 - accuracy: 0.4375 - val_loss: 0.6967 - val_accuracy: 0.4500\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6959 - accuracy: 0.5000 - val_loss: 0.6954 - val_accuracy: 0.4500\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 102s 5s/step - loss: 0.6922 - accuracy: 0.5063 - val_loss: 0.6969 - val_accuracy: 0.4500\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 103s 5s/step - loss: 0.6890 - accuracy: 0.5625 - val_loss: 0.6945 - val_accuracy: 0.4500\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 103s 5s/step - loss: 0.6958 - accuracy: 0.5500 - val_loss: 0.6927 - val_accuracy: 0.5500\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 103s 5s/step - loss: 0.6968 - accuracy: 0.5063 - val_loss: 0.6966 - val_accuracy: 0.4500\n",
            "Final Training Accuracy: 50.63%\n",
            "Final Validation Accuracy: 45.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, GlobalAveragePooling2D, TimeDistributed, LSTM, Dropout, Input, Attention, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths to datasets\n",
        "real_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-real'\n",
        "fake_videos_path = '/content/drive/MyDrive/UMASS_D/First Semsters/Digital forensics/Celeb-DF/Celeb-synthesis'\n",
        "\n",
        "# Function to extract frames from videos\n",
        "def extract_frames(video_path, max_frames=30, resize=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, resize)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    if len(frames) < max_frames:\n",
        "        while len(frames) < max_frames:\n",
        "            frames.append(np.zeros((224, 224, 3), dtype=np.uint8))  # Padding if frames < max_frames\n",
        "    return np.array(frames)\n",
        "\n",
        "# Load data\n",
        "def load_data(data_dir, label, max_videos=100):\n",
        "    videos, labels = [], []\n",
        "    for i, filename in enumerate(os.listdir(data_dir)):\n",
        "        if i == max_videos:\n",
        "            break\n",
        "        filepath = os.path.join(data_dir, filename)\n",
        "        frames = extract_frames(filepath)\n",
        "        if len(frames) == 30:  # Check that each video has 30 frames\n",
        "            videos.append(frames)\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            print(f\"Skipped {filename}: Not enough frames or corrupted video.\")\n",
        "    return np.array(videos), np.array(labels)\n",
        "\n",
        "# Load real and fake video datasets\n",
        "real_videos, real_labels = load_data(real_videos_path, label=0)\n",
        "fake_videos, fake_labels = load_data(fake_videos_path, label=1)\n",
        "\n",
        "# Combine and shuffle the dataset\n",
        "X = np.concatenate((real_videos, fake_videos))\n",
        "y = np.concatenate((real_labels, fake_labels))\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model with CNN and Attention Mechanism\n",
        "def build_attention_cnn_model():\n",
        "    input_layer = Input(shape=(30, 224, 224, 3))  # 30 frames, 224x224 resolution, 3 channels\n",
        "\n",
        "    # CNN feature extractor for each frame\n",
        "    cnn = TimeDistributed(Conv2D(32, (3, 3), activation='relu'))(input_layer)\n",
        "    cnn = TimeDistributed(Conv2D(64, (3, 3), activation='relu'))(cnn)\n",
        "    cnn = TimeDistributed(GlobalAveragePooling2D())(cnn)  # Pooling to reduce spatial dimensions\n",
        "\n",
        "    # Apply Attention mechanism\n",
        "    attention = Attention()([cnn, cnn])  # Self-attention (query and value are the same)\n",
        "\n",
        "    # Flatten the attention output to feed into LSTM\n",
        "    attention = TimeDistributed(Flatten())(attention)\n",
        "\n",
        "    # Apply LSTM to capture temporal dependencies between frames\n",
        "    x = LSTM(64)(attention)\n",
        "\n",
        "    # Dense and Dropout layers for classification\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer for binary classification\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model(inputs=input_layer, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_attention_cnn_model()\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate model performance\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Final Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tAUTqHTell0",
        "outputId": "45f26f56-769b-46aa-e0ba-49616c7b150a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - 727s 36s/step - loss: 0.7363 - accuracy: 0.5375 - val_loss: 0.6911 - val_accuracy: 0.5500\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 720s 36s/step - loss: 0.7110 - accuracy: 0.5250 - val_loss: 0.6912 - val_accuracy: 0.6250\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 720s 36s/step - loss: 0.7057 - accuracy: 0.5688 - val_loss: 0.7008 - val_accuracy: 0.4500\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 722s 36s/step - loss: 0.7308 - accuracy: 0.5063 - val_loss: 0.7102 - val_accuracy: 0.4500\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 721s 36s/step - loss: 0.7100 - accuracy: 0.4375 - val_loss: 0.6994 - val_accuracy: 0.3750\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 722s 36s/step - loss: 0.7084 - accuracy: 0.4812 - val_loss: 0.6924 - val_accuracy: 0.5750\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 720s 36s/step - loss: 0.7119 - accuracy: 0.5188 - val_loss: 0.6965 - val_accuracy: 0.4500\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 721s 36s/step - loss: 0.7052 - accuracy: 0.4938 - val_loss: 0.6982 - val_accuracy: 0.4750\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 722s 36s/step - loss: 0.6921 - accuracy: 0.5375 - val_loss: 0.6883 - val_accuracy: 0.7000\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 720s 36s/step - loss: 0.6931 - accuracy: 0.4688 - val_loss: 0.6987 - val_accuracy: 0.4500\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 721s 36s/step - loss: 0.6818 - accuracy: 0.5437 - val_loss: 0.7091 - val_accuracy: 0.4500\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 722s 36s/step - loss: 0.7165 - accuracy: 0.4375 - val_loss: 0.6951 - val_accuracy: 0.3250\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 721s 36s/step - loss: 0.7024 - accuracy: 0.4563 - val_loss: 0.6964 - val_accuracy: 0.4500\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 723s 36s/step - loss: 0.6932 - accuracy: 0.5063 - val_loss: 0.7030 - val_accuracy: 0.4500\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 722s 36s/step - loss: 0.6961 - accuracy: 0.4750 - val_loss: 0.6827 - val_accuracy: 0.5500\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 721s 36s/step - loss: 0.6917 - accuracy: 0.5375 - val_loss: 0.6944 - val_accuracy: 0.4500\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 721s 36s/step - loss: 0.7022 - accuracy: 0.5437 - val_loss: 0.6950 - val_accuracy: 0.4500\n",
            "Epoch 18/100\n",
            " 8/20 [===========>..................] - ETA: 6:50 - loss: 0.7090 - accuracy: 0.5156"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}